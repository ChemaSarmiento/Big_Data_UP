# Laboratorio de Big Data: Dataproc, Hive, Jupyter y Zeppelin

Este repositorio contiene la gu칤a paso a paso para desplegar un entorno de an치lisis de datos a gran escala en Google Cloud Platform (GCP). El cl칰ster est치 configurado para procesar archivos de m치s de 20GB de manera eficiente.

## 游늶 Requisitos Previos

* Una cuenta activa en Google Cloud Platform.
* Un proyecto creado con la facturaci칩n habilitada.
* Un Bucket en Cloud Storage con los archivos de datos (ej. archivos CSV/JSON de 14GB y 2GB).

---

## 游 1. Creaci칩n del Cl칰ster desde Cloud Shell

Abre **Cloud Shell** y ejecuta los siguientes bloques de c칩digo.

### A) Definici칩n de Variables
Configura el entorno seg칰n tu proyecto.

```bash
export PROJECT_ID=$(gcloud config get-value project)
export REGION=us-central1
export ZONE=us-central1-a
export CLUSTER_NAME=hive-learning-cluster```

gcloud dataproc clusters create $CLUSTER_NAME \
    --project=$PROJECT_ID \
    --region=$REGION \
    --zone=$ZONE \
    --image-version=2.1-ubuntu20 \
    --master-machine-type n2-standard-2 \
    --master-boot-disk-size 100 \
    --num-workers 2 \
    --worker-machine-type n2-standard-2 \
    --worker-boot-disk-size 100 \
    --optional-components JUPYTER,ZEPPELIN \
    --enable-component-gateway \
    --scopes '[https://www.googleapis.com/auth/cloud-platform](https://www.googleapis.com/auth/cloud-platform)'

